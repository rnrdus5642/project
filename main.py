import cv2
import numpy as np
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import asyncio
import websockets
import json
import threading
from dataclasses import dataclass
from typing import Optional, Dict, List
from queue import Queue
import os
import urllib.request

# ============================================================
# BlendShape ë°ì´í„° í´ë˜ìŠ¤
# ============================================================
@dataclass
class BlendShapeData:
    # ê³ ê°œ íšŒì „
    head_x: float = 0.0
    head_y: float = 0.0
    head_z: float = 0.0
    
    # ëˆˆ
    eye_left_open: float = 1.0
    eye_right_open: float = 1.0
    eye_left_x: float = 0.0
    eye_left_y: float = 0.0
    eye_right_x: float = 0.0
    eye_right_y: float = 0.0
    
    # ì…
    mouth_open: float = 0.0
    mouth_smile: float = 0.0
    
    # ëˆˆì¹
    brow_left_y: float = 0.0
    brow_right_y: float = 0.0
    
    # ì¶”ê°€ í‘œì •
    cheek_puff: float = 0.0
    tongue_out: float = 0.0


# ============================================================
# ëœë“œë§ˆí¬ ì¸ë±ìŠ¤
# ============================================================
class LandmarkIndex:
    NOSE_TIP = 1
    FOREHEAD = 10
    CHIN = 152
    LEFT_CHEEK = 234
    RIGHT_CHEEK = 454
    LEFT_EYE_OUTER = 33
    RIGHT_EYE_OUTER = 263


# ============================================================
# BlendShape ê³„ì‚°ê¸° (face_landmarker.task ì‚¬ìš©)
# ============================================================
class BlendShapeCalculator:
    def __init__(self, model_path: str = "face_landmarker.task"):
        self.model_path = model_path
        self._ensure_model()
        
        # FaceLandmarker ì˜µì…˜ ì„¤ì •
        options = vision.FaceLandmarkerOptions(
            base_options=python.BaseOptions(model_asset_path=self.model_path),
            running_mode=vision.RunningMode.IMAGE,
            output_face_blendshapes=True,
            output_facial_transformation_matrixes=True,
            num_faces=1
        )
        
        self.landmarker = vision.FaceLandmarker.create_from_options(options)
        
        # ìŠ¤ë¬´ë”©
        self.prev_data = BlendShapeData()
        self.smoothing_factor = 0.4  # 0.0 ~ 1.0 (ë†’ì„ìˆ˜ë¡ ë¹ ë¥¸ ë°˜ì‘)
        
        print("âœ“ FaceLandmarker ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _ensure_model(self):
        """ëª¨ë¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ"""
        if not os.path.exists(self.model_path):
            print("ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            url = "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
            urllib.request.urlretrieve(url, self.model_path)
            print("âœ“ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    
    def _get_blendshape(self, blendshapes: list, name: str) -> float:
        """BlendShape ê°’ ê°€ì ¸ì˜¤ê¸°"""
        for bs in blendshapes:
            if bs.category_name == name:
                return bs.score
        return 0.0
    
    def _calculate_head_rotation(self, landmarks, w: int, h: int) -> tuple:
        """ê³ ê°œ íšŒì „ ê³„ì‚°"""
        def get_point(idx):
            lm = landmarks[idx]
            return (lm.x * w, lm.y * h, lm.z * w)
        
        nose = get_point(LandmarkIndex.NOSE_TIP)
        left_cheek = get_point(LandmarkIndex.LEFT_CHEEK)
        right_cheek = get_point(LandmarkIndex.RIGHT_CHEEK)
        forehead = get_point(LandmarkIndex.FOREHEAD)
        chin = get_point(LandmarkIndex.CHIN)
        left_eye = get_point(LandmarkIndex.LEFT_EYE_OUTER)
        right_eye = get_point(LandmarkIndex.RIGHT_EYE_OUTER)
        
        # Yaw (ì¢Œìš°)
        total_width = right_cheek[0] - left_cheek[0]
        if total_width > 0:
            left_dist = nose[0] - left_cheek[0]
            right_dist = right_cheek[0] - nose[0]
            yaw = ((left_dist - right_dist) / total_width) * 30
        else:
            yaw = 0
        
        # Pitch (ìƒí•˜)
        face_height = chin[1] - forehead[1]
        if face_height > 0:
            nose_ratio = (nose[1] - forehead[1]) / face_height
            pitch = (nose_ratio - 0.35) * 50
        else:
            pitch = 0
        
        # Roll (ê¸°ìš¸ì„)
        eye_diff = left_eye[1] - right_eye[1]
        eye_width = right_eye[0] - left_eye[0]
        if eye_width > 0:
            roll = np.degrees(np.arctan2(eye_diff, eye_width))
        else:
            roll = 0
        
        return (
            np.clip(yaw, -30, 30),
            np.clip(pitch, -30, 30),
            np.clip(roll, -30, 30)
        )
    
    def _smooth(self, current: float, previous: float) -> float:
        """ê°’ ìŠ¤ë¬´ë”©"""
        return previous + (current - previous) * self.smoothing_factor
    
    def process_frame(self, frame: np.ndarray) -> Optional[BlendShapeData]:
        """í”„ë ˆì„ ì²˜ë¦¬ ë° BlendShape ì¶”ì¶œ"""
        h, w = frame.shape[:2]
        
        # BGR â†’ RGB
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # MediaPipe Image ìƒì„±
        mp_image = mp.Image(
            image_format=mp.ImageFormat.SRGB,
            data=rgb
        )
        
        # ì–¼êµ´ ê°ì§€
        result = self.landmarker.detect(mp_image)
        
        if not result.face_landmarks:
            return None
        
        landmarks = result.face_landmarks[0]
        
        # ê³ ê°œ íšŒì „ ê³„ì‚°
        head_x, head_y, head_z = self._calculate_head_rotation(landmarks, w, h)
        
        # BlendShape ì¶”ì¶œ
        if result.face_blendshapes:
            bs = result.face_blendshapes[0]
            
            # ëˆˆ ê¹œë¹¡ì„ (1ì—ì„œ ë¹¼ì„œ ì—´ë¦¼ ì •ë„ë¡œ ë³€í™˜)
            eye_left_open = 1.0 - self._get_blendshape(bs, "eyeBlinkLeft")
            eye_right_open = 1.0 - self._get_blendshape(bs, "eyeBlinkRight")
            
            # ì‹œì„  ë°©í–¥
            eye_left_x = (self._get_blendshape(bs, "eyeLookOutLeft") - 
                         self._get_blendshape(bs, "eyeLookInLeft"))
            eye_left_y = (self._get_blendshape(bs, "eyeLookDownLeft") - 
                         self._get_blendshape(bs, "eyeLookUpLeft"))
            eye_right_x = (self._get_blendshape(bs, "eyeLookInRight") - 
                          self._get_blendshape(bs, "eyeLookOutRight"))
            eye_right_y = (self._get_blendshape(bs, "eyeLookDownRight") - 
                          self._get_blendshape(bs, "eyeLookUpRight"))
            
            # ì…
            mouth_open = self._get_blendshape(bs, "jawOpen")
            mouth_smile = (self._get_blendshape(bs, "mouthSmileLeft") + 
                          self._get_blendshape(bs, "mouthSmileRight")) / 2
            
            # ëˆˆì¹
            brow_left_y = (self._get_blendshape(bs, "browOuterUpLeft") - 
                          self._get_blendshape(bs, "browDownLeft"))
            brow_right_y = (self._get_blendshape(bs, "browOuterUpRight") - 
                           self._get_blendshape(bs, "browDownRight"))
            
            # ì¶”ê°€ í‘œì •
            cheek_puff = (self._get_blendshape(bs, "cheekPuff"))
            tongue_out = self._get_blendshape(bs, "tongueOut")
        else:
            eye_left_open = eye_right_open = 1.0
            eye_left_x = eye_left_y = eye_right_x = eye_right_y = 0.0
            mouth_open = mouth_smile = 0.0
            brow_left_y = brow_right_y = 0.0
            cheek_puff = tongue_out = 0.0
        
        # ìŠ¤ë¬´ë”© ì ìš©
        data = BlendShapeData(
            head_x=self._smooth(head_x, self.prev_data.head_x),
            head_y=self._smooth(head_y, self.prev_data.head_y),
            head_z=self._smooth(head_z, self.prev_data.head_z),
            eye_left_open=self._smooth(eye_left_open, self.prev_data.eye_left_open),
            eye_right_open=self._smooth(eye_right_open, self.prev_data.eye_right_open),
            eye_left_x=self._smooth(eye_left_x, self.prev_data.eye_left_x),
            eye_left_y=self._smooth(eye_left_y, self.prev_data.eye_left_y),
            eye_right_x=self._smooth(eye_right_x, self.prev_data.eye_right_x),
            eye_right_y=self._smooth(eye_right_y, self.prev_data.eye_right_y),
            mouth_open=self._smooth(mouth_open, self.prev_data.mouth_open),
            mouth_smile=self._smooth(mouth_smile, self.prev_data.mouth_smile),
            brow_left_y=self._smooth(brow_left_y, self.prev_data.brow_left_y),
            brow_right_y=self._smooth(brow_right_y, self.prev_data.brow_right_y),
            cheek_puff=self._smooth(cheek_puff, self.prev_data.cheek_puff),
            tongue_out=self._smooth(tongue_out, self.prev_data.tongue_out),
        )
        
        self.prev_data = data
        return data
    
    def get_raw_blendshapes(self, frame: np.ndarray) -> Dict[str, float]:
        """ëª¨ë“  BlendShape ì›ë³¸ ê°’ ë°˜í™˜ (ë””ë²„ê·¸ìš©)"""
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)
        result = self.landmarker.detect(mp_image)
        
        if result.face_blendshapes:
            return {bs.category_name: bs.score for bs in result.face_blendshapes[0]}
        return {}
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        self.landmarker.close()


# ============================================================
# VTube Studio API
# ============================================================
class VTubeStudioAPI:
    def __init__(self, host: str = "localhost", port: int = 8001):
        self.uri = f"ws://{host}:{port}"
        self.websocket = None
        self.authenticated = False
        self.plugin_name = "MediaPipe Tracker"
        self.plugin_developer = "Python"
        self.auth_token = None
        self.request_id = 0
    
    async def connect(self) -> bool:
        try:
            self.websocket = await websockets.connect(self.uri)
            print(f"âœ“ VTube Studio ì—°ê²°: {self.uri}")
            return True
        except Exception as e:
            print(f"âœ— ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    async def _send(self, msg_type: str, data: dict = None) -> dict:
        self.request_id += 1
        request = {
            "apiName": "VTubeStudioPublicAPI",
            "apiVersion": "1.0",
            "requestID": f"req_{self.request_id}",
            "messageType": msg_type,
            "data": data or {}
        }
        await self.websocket.send(json.dumps(request))
        return json.loads(await self.websocket.recv())
    
    async def authenticate(self, saved_token: str = None) -> bool:
        # í† í° ìš”ì²­
        if not saved_token:
            resp = await self._send("AuthenticationTokenRequest", {
                "pluginName": self.plugin_name,
                "pluginDeveloper": self.plugin_developer,
                "pluginIcon": ""
            })
            self.auth_token = resp.get("data", {}).get("authenticationToken")
            if self.auth_token:
                print("â³ VTube Studioì—ì„œ í”ŒëŸ¬ê·¸ì¸ì„ ìŠ¹ì¸í•´ì£¼ì„¸ìš”...")
                await asyncio.sleep(3)
        else:
            self.auth_token = saved_token
        
        # ì¸ì¦
        resp = await self._send("AuthenticationRequest", {
            "pluginName": self.plugin_name,
            "pluginDeveloper": self.plugin_developer,
            "authenticationToken": self.auth_token
        })
        
        self.authenticated = resp.get("data", {}).get("authenticated", False)
        print(f"{'âœ“' if self.authenticated else 'âœ—'} ì¸ì¦ {'ì„±ê³µ' if self.authenticated else 'ì‹¤íŒ¨'}")
        return self.authenticated
    
    async def send_tracking_data(self, data: BlendShapeData):
        if not self.authenticated:
            return

        # ëˆˆì¹ ì „ì²´ (ì¢Œìš° í‰ê· )
        brows = (data.brow_left_y + data.brow_right_y) * 0.5

        params = [
            # ======================
            # ì–¼êµ´ ê°ë„
            # ======================
            {"id": "FaceAngleX", "value": data.head_x},  # Pitch
            {"id": "FaceAngleY", "value": data.head_y},  # Yaw
            {"id": "FaceAngleZ", "value": data.head_z},  # Roll

            # ======================
            # ëˆˆ
            # ======================
            {"id": "EyeOpenLeft",  "value": data.eye_left_open},
            {"id": "EyeOpenRight", "value": data.eye_right_open},

            {"id": "EyeLeftX",  "value": data.eye_left_x},
            {"id": "EyeLeftY",  "value": data.eye_left_y},
            {"id": "EyeRightX", "value": data.eye_right_x},
            {"id": "EyeRightY", "value": data.eye_right_y},

            # ======================
            # ëˆˆì¹
            # ======================
            {"id": "Brows",       "value": brows},
            {"id": "BrowLeftY",   "value": data.brow_left_y},
            {"id": "BrowRightY",  "value": data.brow_right_y},

            # ======================
            # ì… & í‘œì •
            # ======================
            {"id": "MouthOpen",   "value": data.mouth_open},
            {"id": "MouthSmile",  "value": data.mouth_smile},
            {"id": "CheekPuff",   "value": data.cheek_puff},
            {"id": "TongueOut",   "value": data.tongue_out},
        ]

        await self._send(
            "InjectParameterDataRequest",
            {
                "faceFound": True,
                "mode": "set",
                "parameterValues": params
            }
        )


# ============================================================
# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# ============================================================
class FaceTrackingApp:
    def __init__(self):
        self.calculator = BlendShapeCalculator()
        self.vts = VTubeStudioAPI()
        self.running = False
        self.data_queue = Queue()
        self.token_file = "vts_token.txt"
    
    def _load_token(self) -> Optional[str]:
        try:
            with open(self.token_file, 'r') as f:
                return f.read().strip()
        except:
            return None
    
    def _save_token(self, token: str):
        with open(self.token_file, 'w') as f:
            f.write(token)
    
    async def _vts_loop(self):
        """VTube Studio í†µì‹  ë£¨í”„"""
        if not await self.vts.connect():
            print("VTube Studio ì—°ê²° ì‹¤íŒ¨ - íŠ¸ë˜í‚¹ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤")
            return
        
        token = self._load_token()
        if await self.vts.authenticate(token):
            if self.vts.auth_token:
                self._save_token(self.vts.auth_token)
        else:
            return
        
        while self.running:
            try:
                if not self.data_queue.empty():
                    data = self.data_queue.get_nowait()
                    await self.vts.send_tracking_data(data)
                await asyncio.sleep(0.016)  # ~60fps
            except Exception as e:
                print(f"VTS ì˜¤ë¥˜: {e}")
                break
        
        await self.vts.close()
    
    def _camera_loop(self):
        """ì¹´ë©”ë¼ ìº¡ì²˜ ë£¨í”„"""
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        if not cap.isOpened():
            print("âœ— ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            self.running = False
            return
        
        print("âœ“ ì¹´ë©”ë¼ ì‹œì‘")
        print("\n[ì¡°ì‘ë²•]")
        print("  q: ì¢…ë£Œ")
        print("  d: ë””ë²„ê·¸ ì •ë³´ í† ê¸€")
        print("  e: ëˆˆ í‘œì‹œê¸° í† ê¸€")
        print("  s: ìŠ¤ë¬´ë”© ì¡°ì ˆ (+0.1)")
        print("  b: ëª¨ë“  BlendShape ì¶œë ¥")
        
        show_debug = True
        show_eyes = True
        
        while self.running:
            ret, frame = cap.read()
            if not ret:
                continue
            
            # ê±°ìš¸ ëª¨ë“œ
            frame = cv2.flip(frame, 1)
            
            # BlendShape ê³„ì‚°
            data = self.calculator.process_frame(frame)
            
            if data:
                # VTSë¡œ ì „ì†¡
                if self.data_queue.qsize() < 3:
                    self.data_queue.put(data)
            else:
                cv2.putText(frame, "No face detected", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            cv2.imshow("MediaPipe Face Tracking (q: quit)", frame)
            
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                self.running = False
            elif key == ord('s'):
                self.calculator.smoothing_factor = min(1.0, 
                    self.calculator.smoothing_factor + 0.1)
                if self.calculator.smoothing_factor > 0.95:
                    self.calculator.smoothing_factor = 0.1
                print(f"ìŠ¤ë¬´ë”©: {self.calculator.smoothing_factor:.1f}")
            elif key == ord('b'):
                # ëª¨ë“  BlendShape ì¶œë ¥
                bs = self.calculator.get_raw_blendshapes(frame)
                print("\n=== All BlendShapes ===")
                for name, value in sorted(bs.items()):
                    if value > 0.01:
                        print(f"  {name}: {value:.3f}")
        
        cap.release()
        cv2.destroyAllWindows()
        self.calculator.close()
    
    def run(self):
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰"""
        print("=" * 50)
        print("  MediaPipe Face Tracker â†’ VTube Studio")
        print("=" * 50)
        
        self.running = True
        
        # VTS í†µì‹  ìŠ¤ë ˆë“œ
        def run_vts():
            asyncio.run(self._vts_loop())
        
        vts_thread = threading.Thread(target=run_vts, daemon=True)
        vts_thread.start()
        
        # ì¹´ë©”ë¼ ë£¨í”„ (ë©”ì¸ ìŠ¤ë ˆë“œ)
        self._camera_loop()
        
        self.running = False
        print("\ní”„ë¡œê·¸ë¨ ì¢…ë£Œ")

# ============================================================
# ì‹¤í–‰
# ============================================================
if __name__ == "__main__":
    import sys
    FaceTrackingApp().run()
